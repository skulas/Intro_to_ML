{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn\n",
    "A notebook dedicated to learn scikit learn and test stuff.<br>\n",
    "Following this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import HTML\n",
    "#tutorialPage = HTML('https://www.datacamp.com/community/tutorials/machine-learning-python')\n",
    "#tutorialPage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the tutorial in this link:\n",
    "[Tutorial](https://www.datacamp.com/community/tutorials/machine-learning-python)<br>\n",
    "Load a practice dataset predefined in scikit, even if it actually points to ics.uci.edu ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "# print(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "# Load the training data as a Pandas DataFrame\n",
    "#digits = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\", header=None)\n",
    "# print(digits)\n",
    "#digits_test = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glance at the data organization\n",
    "print('keys')\n",
    "print(digits.keys())\n",
    "print('/***/')\n",
    "print('target')\n",
    "print(digits.target)\n",
    "print(digits.target_names)\n",
    "print('description')\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking into the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_data = digits.data\n",
    "\n",
    "print('Data Shape')\n",
    "print(digits_data.shape)\n",
    "\n",
    "print('Possible Targets')\n",
    "digits_target = digits.target\n",
    "print(digits_target)\n",
    "\n",
    "print('Number of Unique Targets')\n",
    "num_digits = len(np.unique(digits_target))\n",
    "print(num_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have ** *1797 Samples* ** of ** *64 features* ** and there are ** *10 classes* ** for classification<br>\n",
    "**The Images**<br>\n",
    "We also have 1797 8x8 images. We're gonna reshape them to make sure they match the trainig data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_images = digits.images.reshape((1797,64))\n",
    "print('Let''s compare the training data with the reshaped images and check for full consistency:')\n",
    "print('Fully consistent' if np.all(reshaped_images == digits_data) else 'Unconsistency detected between images and data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing the images for some intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Image of 7x7 inches\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "# Define subplots\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.075, wspace=0.075)\n",
    "\n",
    "# Draw each digit into a separated sub_plot\n",
    "for i in range(64):\n",
    "    # Init the Subplots: Add a subplot grid of 8x8 at the i+1 position\n",
    "    ax = fig.add_subplot(8,8,i+1, xticks=[], yticks=[])\n",
    "    # Display an image at the i_th position\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    # Label the image with the target value (the digit that should be drawed)\n",
    "    ax.text(0, 7, str(digits_target[i]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and for curiosity, other way to print them by merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joing images and labels into a single list\n",
    "images_and_labeles = list(zip(digits.images, digits_target))\n",
    "\n",
    "# Print the first row above\n",
    "for ix, (image, lbl) in enumerate(images_and_labeles[:8]):\n",
    "    # Build a 4x2 subplots table. Subplot index is 1 based.\n",
    "    plt.subplot(2, 4, ix + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r,interpolation='nearest')\n",
    "    plt.title('Training' + str(lbl))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "When there are too many features thus too many dimensions, like ** 64 ** in our example, visualizing the data is impossible.<br>\n",
    "Sometimes too many features separate away the samples in a maner that is hard to find something in common between samples that share the same label.<br>\n",
    "To visualize data represented by a high number of dimensions we can use of the ** * Dimensionality Reduction * ** techniques, we'll use here *PCA*: **Principal Component Analysis**.<br>\n",
    "The idea in PCA is to find a linear combination of the two variables that contains most of the information. This new variable or *principal component* can replace the two original variables.\n",
    "#### The Method\n",
    "A linear transformation method that yeilds the directions (principal compoents) that maximazie the variance of the data.<br>\n",
    "**Variance:** How far a set f data points lie apart.<br>\n",
    "For more details you can read [Introduction to PCA](http://www.lauradhamilton.com/introduction-to-principal-component-analysis-pca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Create a randomized PCA model that takes 2 components\n",
    "randomized_pca = PCA(svd_solver='randomized',n_components=2)\n",
    "# Fit and transform the data to the PCA model\n",
    "reduced_data_rpca = randomized_pca.fit_transform(digits_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's also generate a *full PCA* (Runs exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select the components by postprocessing) rather than the random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full PCA model that takes 2 components\n",
    "pca = PCA(svd_solver='full', n_components=2)\n",
    "# now fit and transform the data with the full PCA\n",
    "reduced_data_pca = pca.fit_transform(digits_data)\n",
    "\n",
    "# let's see\n",
    "print('Randomized PCA data shape:' + str(reduced_data_rpca.shape))\n",
    "print('Full PCA data shape:' + str(reduced_data_pca.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having 2D data, we can plot it!<br>\n",
    "Let's see first if the random PCA reveals the distribution of the different labels and if we can clearly separate the instances from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(pca_data, titleStr):\n",
    "    colors = ['black', 'blue', 'green', 'yellow', 'red', 'purple', 'lime', 'cyan', 'orange', 'gray']\n",
    "    markers = ['$0$', '$1$', '$2$', '$3$', '$4$','$5$', '$6$', '$7$', '$8$', '$9$']\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    for dtix in range(len(colors)):\n",
    "        x = pca_data[:, 0][digits_target == dtix]\n",
    "        y = pca_data[:, 1][digits_target == dtix]\n",
    "        plt.scatter(x,y, c=colors[dtix], marker=markers[dtix])\n",
    "\n",
    "    plt.legend(digits.target_names, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title(titleStr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(reduced_data_rpca, 'Random PCA Scatter Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare it with the Full PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(reduced_data_pca, 'Full PCA Scatter Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full vs Random ... <br>\n",
    "... exactly the same??<br>\n",
    "This means there is no need to scan the entire feature set. In such case, better use the Random as it's quicker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosing the correct algorithm\n",
    "Once we understand our data, we need to use the best algorithm for our problem.<br>\n",
    "Use this map to think about what would be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.Image(url='http://scikit-learn.org/stable/_static/ml_map.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "Before modeling the data we need to prepre it first.<br>\n",
    "### Data Normalization\n",
    "Standardize the data.<br>\n",
    "In the digits case we could make use of the `scale()` method<br>\n",
    "By scaling the data, you shift the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "data = scale(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into Trainig and Testing Sets\n",
    "The division of the data set into a test and a training sets is disjoint: the most common splitting choice is to take 2/3 of your original data set as the training set, while the 1/3 that remains will compose the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into Train and Test sets\n",
    "X_train, X_test, Y_train, Y_test, images_train, images_test = train_test_split(data, digits_target, digits.images, \\\n",
    "                                                                               test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `random_state` has the value *42* assigned to it. With this argument, you can guarantee that your split will always be **the same**. That is particularly handy if you want reproducible results.<br>\n",
    "Inspect the number of samples in the sets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separator_line():\n",
    "    print(20*'____')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X_train.shape\n",
    "print('Number of training samples')\n",
    "print(str(n_samples))\n",
    "print('Number of training features')\n",
    "print(str(n_features))\n",
    "separator_line()\n",
    "n_digits = len(np.unique(Y_train))\n",
    "print('Number of Training labels (unique)')\n",
    "print(str(n_digits))\n",
    "print('Total number of Training labels')\n",
    "print(str(len(Y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the data\n",
    "Find those clusters of the training set. Use `KMeans()` from the `cluster` module to set up your model. You’ll see that there are three arguments that are passed to this method: `init`, `n_clusters` and the `random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "# Creating the KMeans model\n",
    "clf = cluster.KMeans(init='k-means++', n_clusters=n_digits, random_state=42) #, n_init=9) #same random_state as when splitting\n",
    "\n",
    "# Fit the trainig data X_train to the model\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing is, how do we set the centers of the clusters. The number of centroid is set by `n_clusters`.<br>\n",
    "The initial set of cluster centers can have a big effect on the clusters that are eventually found.\n",
    "To deal with this effect, we usually try several initial sets in multiple runs and select the set of clusters with the minimum sum of the squared errors (SSE). In other words, you want to minimize the distance of each point in the cluster to the mean or centroid of that cluster.<br>\n",
    "By adding the `n-init` argument to KMeans(), you can determine how many different centroid configurations the algorithm will try. The thing is that more configuratins **doesn't mean chosing the best configuration** ...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(8,3)\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "fig.suptitle('Cluster Center Images', fontsize=14, fontweight='bold')\n",
    "\n",
    "for imix in range(10):\n",
    "    # Initialize subplots grid of 2x5, ONE based\n",
    "    ax = fig.add_subplot(2,5,imix + 1)\n",
    "    \n",
    "    ax.imshow(clf.cluster_centers_[imix].reshape((8,8)), cmap=plt.cm.binary)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Predict the labels of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=clf.predict(X_test)\n",
    "num2print = 25\n",
    "print(Y_pred[:num2print])\n",
    "print(Y_test[:num2print])\n",
    "separator_line()\n",
    "print('Cluster Centers Shape:')\n",
    "clf.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "# Create an isomap and fit the digits data to the isomap\n",
    "X_iso = Isomap(n_neighbors=n_digits).fit_transform(X_train)\n",
    "\n",
    "# Compute cluster centers and predict clster index for each sample\n",
    "clusters = clf.fit_predict(X_train)\n",
    "\n",
    "# Create a plot with subplots in a grid of 1x2\n",
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "# Add the scatter plots\n",
    "ax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=clusters)\n",
    "ax[0].set_title('Predicted Training Lables')\n",
    "ax[1].scatter(X_iso[:,0], X_iso[:,1], c=Y_train)\n",
    "ax[1].set_title('Actual Taining Labels')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the same using the reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "clusters_pca = clf.fit_predict(X_pca) \n",
    "clusters = clf.fit_predict(X_train) \n",
    "print('Clusters Shape: ' + str(clusters.shape))\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "fig.suptitle('Predicted Versus Training PCA Labels', fontsize=14, fontweight='bold')\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "ax[0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters)\n",
    "ax[0].set_title('Predicted Training Labels')\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_pca)\n",
    "ax[1].set_title('Predicted PCA Training Labels')\n",
    "ax[2].scatter(X_pca[:, 0], X_pca[:, 1], c=Y_train)\n",
    "ax[2].set_title('Actual Training Labels')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eveluating the Clustered Model\n",
    "Let's evaluate the peformance (which look pretty lame) of our model.<br>\n",
    "#### The Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... crap. Only 5 was predicted correctly 41 times.<br>\n",
    "Let's look into some *cluster cuality metics*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import homogeneity_score, completeness_score,\\\n",
    "v_measure_score,adjusted_rand_score, adjusted_mutual_info_score,silhouette_score\n",
    "\n",
    "print('% 9s' % 'inertia\\thomo\\tcomple\\tv-means\\tARI\\tAMI\\tsilhouette')\n",
    "print('%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f' %(clf.inertia_,\n",
    "                                                homogeneity_score(Y_test, Y_pred),\n",
    "                                                completeness_score(Y_test, Y_pred),\n",
    "                                                v_measure_score(Y_test, Y_pred),\n",
    "                                                adjusted_rand_score(Y_test, Y_pred),\n",
    "                                                adjusted_mutual_info_score(Y_test, Y_pred),\n",
    "                                                silhouette_score(X_test, Y_pred, metric='euclidean')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **homogeneity** score tells you to what extent all of the clusters contain only data points which are members of a single class.\n",
    "* The **completeness** score measures the extent to which all of the data points that are members of a given class are also elements of the same cluster.\n",
    "* The **V-measure** score is the harmonic mean between homogeneity and completeness.\n",
    "* The **Adjusted Rand Info (ARI)** score measures the similarity between two clusterings and considers all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "* The **Adjusted Mutual Info (AMI)** score is used to compare clusters. It measures the similarity between the data points that are in the clusterings, accounting for chance groupings and takes a maximum value of 1 when clusterings are equivalent.\n",
    "* The **silhouette** score measures how similar an object is to its own cluster compared to other clusters. The silhouette scores ranges from -1 to 1, where a higher value indicates that the object is better matched to its own cluster and worse mached to neighboring clusters. If many points have a high value, the clusteirng configuration is good.<br>\n",
    "\n",
    "\n",
    "... crap indeed. For example:\n",
    "The *silhouette* score is close to 0, which indicates that the sample is on or very close to the decision boundary between two neighboring clusters. This could indicate that the samples could have been assigned to the wrong cluster.<br>\n",
    "Also the *ARI* measure seems to indicate that not all data points in a given cluster are similar and the *completeness* score tells you that there are definitely data points that weren’t put in the right cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need a better predictor the the neigboors ...\n",
    "### Suport Vector Machines !!\n",
    "For KMeans we didn't need the Labels. We just tried to gather samples together into clusters.<br>\n",
    "Let's use the targets together with the training data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svc_model_linear = svm.SVC(gamma=0.001, C=100, kernel='linear') #, verbose=True)\n",
    "\n",
    "# Fit the data to the SVC model\n",
    "fit_result_linear = svc_model_linear.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Estimation\n",
    "A way for finding best values for the parameters `gamma`, `C` and `kernel`\n",
    "It is possible to automatically find good values for the parameters by using tools such as grid search and cross validation.<br>\n",
    "**Grid Search** example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define possibilities\n",
    "parameter_candidates = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]\n",
    "\n",
    "# Create a classfier with the options above\n",
    "clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)\n",
    "\n",
    "# Train \n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Let's see ...\n",
    "print('Best score for training data:', clf.best_score_)\n",
    "print('Best ''C'':', clf.best_estimator_.C)\n",
    "print('Best kernel:', clf.best_estimator_.kernel)\n",
    "print('Best gamma:', clf.best_estimator_.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a winner!<br>\n",
    "kernel - rbf<br>\n",
    "gamma - 0.001<br>\n",
    "C - 1000<br>\n",
    "We'd redefine the SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the classifier to the test data and check the acuracy score\n",
    "accu = clf.score(X_test, Y_test)\n",
    "print('Accuracy of grid search result: ', str(accu))\n",
    "\n",
    "# Train an score a new classfier with the grid search parameters\n",
    "svc_model = svm.SVC(gamma=clf.best_estimator_.gamma, C=clf.best_estimator_.C, kernel=clf.best_estimator_.kernel)\n",
    "fitResult = svc_model.fit(X_train, Y_train)\n",
    "accu = fitResult.score(X_test, Y_test)\n",
    "print('Accuracy of SVM prediction:', accu)\n",
    "accu = fit_result_linear.score(X_test, Y_test)\n",
    "print('Accuracy of Linear SVM prediction:', accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "Now let's check a few predictions to see how good we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = svc_model_linear.predict(X_test)\n",
    "\n",
    "print(Y_predict[:num2print])\n",
    "print(Y_test[:num2print])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_and_predictions = list(zip(images_test, Y_predict))\n",
    "numOfDrawings = 8\n",
    "\n",
    "fig = plt.figure(figsize=(2*numOfDrawings,2))\n",
    "for imix, (image, prediction) in enumerate(images_and_predictions[:numOfDrawings]):\n",
    "    plt.subplot(1, numOfDrawings, imix+1)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Predicted: ' + str(prediction))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how did our model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification report of y_test and predicted')\n",
    "print(metrics.classification_report(Y_test, Y_predict))\n",
    "separator_line()\n",
    "print('Confusion Matrix')\n",
    "print(metrics.confusion_matrix(Y_test, Y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = svc_model_linear.predict(X_train)\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "ax[0].scatter(X_iso[:,0], X_iso[:,1],c=predicted)\n",
    "ax[0].set_title('Predicted Labels')\n",
    "ax[1].scatter(X_iso[:,0], X_iso[:,1],c=Y_train)\n",
    "ax[1].set_title('Actual Labels')\n",
    "\n",
    "fig.suptitle('Predicted vs Actual Labels', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
