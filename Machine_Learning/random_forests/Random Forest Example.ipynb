{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression Example\n",
    "Following [This Tutorial](http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Utilities for scaling, transforming and wrangling data\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Family of models that use Random Forests for regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Cross Validation tools\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metrics for model evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# An alternative to pickle for presistently saving numpy arrays efficiently\n",
    "from sklearn.externals import  joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data about wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    data = pd.read_pickle('./wine_dataset_in_pandas.pkl')\n",
    "    print('Loaded local copy of wines data')\n",
    "except FileNotFoundError:\n",
    "    dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "    data = pd.read_csv(dataset_url, sep=';')\n",
    "    data.to_pickle('./wine_dataset_in_pandas.pkl')\n",
    "    print('Downloaded data from the we and saved it locally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.describe())\n",
    "display(data.head(4))\n",
    "print('Data Size:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 Features<br>\n",
    "1600 Samples<br>\n",
    "**quality** is the target<br>\n",
    "Data features look to be very different, scale wise. _chlorides_ vs _total sulfur dioxide_ for instance<br>\n",
    "**Scaling** will be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split between features and labels\n",
    "y = data.quality # labels\n",
    "X = data.drop('quality', axis=1) # features\n",
    "\n",
    "# Split between training and testing\n",
    "# Predefine a random state (seed) so we can get the same split again.\n",
    "# By stratifying by the target we ensure training set looks similar o the test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)\n",
    "\n",
    "# Scaling all the features\n",
    "X_train_scaled = preprocessing.scale(X_train, axis=0, with_mean=False, with_std=0)\n",
    "display(X_train_scaled[0:2])\n",
    "display(X_train_scaled[2][0:2])\n",
    "display(X_train_scaled[0:5,2])\n",
    "cloriScaledMax = X_train_scaled[:,2].max()\n",
    "cloriScaledMin = X_train_scaled[:,2].min()\n",
    "cloriOriginalMax = X_train.chlorides.max()\n",
    "cloriOriginalMin = X_train.chlorides.min()\n",
    "print('Original Min = ', cloriOriginalMin)\n",
    "print('Scaled Min = ', cloriScaledMin)\n",
    "\n",
    "print('Original MAX = ', cloriOriginalMax)\n",
    "print('Scaled MAX = ', cloriScaledMax)\n",
    "print('='*50)\n",
    "\n",
    "sulDioxScaledMax = X_train_scaled[:,6].max()\n",
    "sulDioxScaledMin = X_train_scaled[:,6].min()\n",
    "sulDioxOriginalMax = X_train['total sulfur dioxide'].max()\n",
    "sulDioxOriginalMin = X_train['total sulfur dioxide'].min()\n",
    "print('Original Min = ', sulDioxOriginalMin)\n",
    "print('Scaled Min = ', sulDioxScaledMin)\n",
    "\n",
    "print('Original MAX = ', sulDioxOriginalMax)\n",
    "print('Scaled MAX = ', sulDioxOriginalMin)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NOTE **\n",
    "\n",
    "Great, but we won't use this scaling.<br>\n",
    "The reason is that we won't be able to perform the exact same transformation on the test set.<br>\n",
    "Sure, we can still scale the test set separately, but we won't be using the same means and standard deviations as we used to transform the training set.<br>\n",
    "In other words, that means it wouldn't be a fair representation of how the model pipeline, include the preprocessing steps, would perform on brand new data.\n",
    "\n",
    "So instead of directly invoking the scale function, we'll be using a feature in Scikit-Learn called the Transformer API.<br>\n",
    "The Transformer API allows you to \"fit\" a preprocessing step using the training data the same way you'd fit a model...\n",
    "...and then use the same transformation on future data sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "display(X_train_scaled.mean(axis=0))\n",
    "display(X_train_scaled.std(axis=0))\n",
    "display(X_train_scaled.min())\n",
    "display(X_train_scaled.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the training set is evenly distributed around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "display(X_test_scaled.mean(axis=0))\n",
    "display(X_test_scaled.std(axis=0))\n",
    "display(X_test_scaled.min())\n",
    "display(X_test_scaled.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled features in the test set are not centered at zero with variance **!= 1**. This is exactly what we'd expect, as we're transforming the test set using the means from the training set, not from the test set itself.<br>\n",
    "In practice, when we set up the cross-validation pipeline, we won't even need to manually fit the Transformer API. Instead, we'll simply declare the class object, like so\n",
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(preprocessing.StandardScaler(), RandomForestRegressor(n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline passes the data through a `StandardScaler` and the output enters the `RandomForestRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "There are two types of parameters we need to worry about: model parameters and hyperparameters. Models parameters can be learned directly from the data (i.e. regression coefficients), while hyperparameters cannot.<br>\n",
    "Hyperparameters express \"higher-level\" structural information about the model, and they are typically set before training the model.\n",
    "#### Random Forest hyperparameters\n",
    "As an example, let's take our random forest for regression:<br>\n",
    "Within each decision tree, the computer can empirically decide where to create branches based on either mean-squared-error (MSE) or mean-absolute-error (MAE). Therefore, the actual branch locations are model parameters.<br>\n",
    "However, the algorithm does not know which of the two criteria, MSE or MAE, that it should use. The algorithm also cannot decide how many trees to include in the forest. These are examples of _hyperparameters_ that the user must set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the tunable hyperparameters\n",
    "display(pipeline.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a pippeline, each hyperparameter is prefixed by the stage of the pipeline `randomforestregressor__` or `standardscaler__`<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "                    'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                    'randomforestregressor__max_depth' : [None, 5, 3, 1]\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_features** : int, float, string or None, optional (default=”auto”)\n",
    "The number of features to consider when looking for the best split:\n",
    "* _int_ then consider max_features features at each split.\n",
    "* _float_ then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n",
    "* _“auto”_ then max_features=n_features.\n",
    "* _“sqrt”_ then max_features=sqrt(n_features).\n",
    "* _“log2”_ then max_features=log2(n_features).\n",
    "* _None_ then max_features=n_features.\n",
    "\n",
    "**max_depth**:<br>\n",
    "If _None_, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model using Cross-Validation (CV) pipeline\n",
    "This is one of the most important skills in all of machine learning because it helps maximizing the model performance while reducing the chance of overfitting.<br>\n",
    "**Cross-validation** is a process for reliably estimating the performance of a method for building a model by training and evaluating your model multiple times using the same method.<br>\n",
    "Practically, that \"method\" is simply a set of hyperparameters in this context.<br>\n",
    "These are the steps for CV:\n",
    "1. Split your data into k equal parts, or \"folds\" (typically k=10).\n",
    "2. Train your model on k-1 folds (e.g. the first 9 folds).\n",
    "3. Evaluate it on the remaining \"hold-out\" fold (e.g. the 10th fold).\n",
    "\n",
    "Perform steps (2) and (3) k times, each time holding out a different fold.<br>\n",
    "Aggregate the performance across all k folds. This is your _performance metric_.\n",
    "\n",
    "<img src=https://elitedatascience.com/wp-content/uploads/2016/12/K-fold_cross_validation_EN.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Motivation**<br>\n",
    "Let's say you want to train a random forest regressor. One of the hyperparameters you must tune is the maximum depth allowed for each decision tree in your forest.\n",
    "How can you decide?\n",
    "That's where cross-validation comes in. Using only your training set, you can use CV to evaluate different hyperparameters and estimate their effectiveness.\n",
    "This allows you to keep your test set \"untainted\" and save it for a true hold-out evaluation when you're finally ready to select a model.\n",
    "For example, you can use CV to tune a random forest model, a linear regression model, and a k-nearest neighbors model, using only the training set. Then, you still have the untainted test set to make your final selection between the model families!\n",
    "\n",
    "**CV \"Pipeline\"**<br>\n",
    "The best practice when performing CV is to include your data preprocessing steps inside the cross-validation loop.<br>\n",
    "This prevents accidentally tainting your training folds with influential data from your test fold.<br>\n",
    "Here's how the CV pipeline looks after including preprocessing steps:\n",
    "\n",
    "1. Split your data into k equal parts, or \"folds\" (typically k=10).\n",
    "2. **Preprocess k-1** training folds.\n",
    "3. Train your model on the same k-1 folds.\n",
    "4. **Preprocess the hold-out** fold using the same transformations from step (2).\n",
    "5. Evaluate your model on the same hold-out fold.\n",
    "6. Perform steps (2) - (5) k times, each time holding out a different fold.\n",
    "5. Aggregate the performance across all k folds. This is your performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easily CVing the pipeline with Scikit is easy\n",
    "# cv is the number of folds to take\n",
    "cvp = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    "\n",
    "# Fit and tune\n",
    "cvp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform CV across the entire _grid_ (i.e. all possible permutaions) of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters for our pipeline:')\n",
    "display(cvp.best_params_)\n",
    "print('Best estimator')\n",
    "display(cvp.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refit on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tunning your hyperparameters appropriately using cross-validation, get a small performance improvement by refitting the model on the entire training set.<br>\n",
    "`GridSearchCV`  will automatically refit the model with the best set of hyperparameters using the entire training set.<br>\n",
    "This functionality is ON by default, but you can confirm it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is fit: \" + str(cvp.refit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model pipeline\n",
    "Predict the test data using the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cvp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluation Metrics\n",
    "Using the metics we previously imported evaluate the model performance\n",
    "\\begin{equation*}\n",
    "R^2 score\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(r2_score(y_test, y_pred))\n",
    "print(\"MSE (Mean Square Error)\")\n",
    "display(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, is this good?\n",
    "They recommend a combination of three strategies to decide if you're satisfied with your model performance.<br>\n",
    "1. Start with the goal of the model. If the model is tied to a business problem, have you successfully solved the problem?\n",
    "2. Look in academic literature to get a sense of the current performance benchmarks for specific types of data.\n",
    "3. Try to find low-hanging fruit in terms of ways to improve your model.\n",
    "4. There are various ways to improve a model. We'll have more guides that go into detail about how to improve model performance, but here are a few quick things to try:\n",
    "    1. Try other regression model families (e.g. regularized regression, boosted trees, etc.).\n",
    "    1. Collect more data if it's cheap to do so.\n",
    "    1. Engineer smarter features after spending more time on exploratory analysis.\n",
    "    1. Speak to a domain expert to get more context (...this is a good excuse to go wine tasting!).\n",
    "\n",
    "As a final note, when you try other families of models, we recommend using the **same training and test** set as you used to fit the random forest model. That's the best way to get a true apples-to-apples comparison between your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Model to the HD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(cvp, 'rf_regressor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loading the model and using it:\n",
    "cvpFromDisk = joblib.load('./rf_regressor.pkl')\n",
    "cvpFromDisk.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
